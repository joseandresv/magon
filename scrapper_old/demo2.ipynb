{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(url, max_retries=3, delay=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'\n",
    "    }\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching the page: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "def extract_article_content(soup):\n",
    "    # Remove unwanted elements like scripts, styles, and comments\n",
    "    for element in soup([\"script\", \"style\", \"meta\", \"noscript\", \"iframe\", \"aside\", \"header\", \"footer\", \"nav\"]):\n",
    "        element.decompose()\n",
    "\n",
    "    # Find the main content container\n",
    "    main_content = None\n",
    "    content_tags = [\"article\", \"div\", \"section\", \"main\"]\n",
    "    for tag in content_tags:\n",
    "        main_content = soup.find(tag, {\"class\": re.compile(\"(article|post|content|entry|body|story)\", re.IGNORECASE)})\n",
    "        if main_content:\n",
    "            break\n",
    "\n",
    "    if main_content is None:\n",
    "        main_content = soup.find(\"body\")\n",
    "\n",
    "    # Extract paragraphs from the main content\n",
    "    paragraphs = main_content.find_all(\"p\")\n",
    "    article_content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "    # If the extracted content is too short, try extracting from the whole page\n",
    "    if len(article_content) < 200:\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        article_content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "    # Remove empty lines and extra whitespace\n",
    "    article_content = re.sub(r\"\\n+\", \"\\n\", article_content).strip()\n",
    "    article_content = re.sub(r\"\\s+\", \" \", article_content)\n",
    "\n",
    "    return article_content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_article_title(soup):\n",
    "    title_element = soup.find('title')\n",
    "    if title_element:\n",
    "        return title_element.get_text(strip=True)\n",
    "\n",
    "    header_element = soup.find('header')\n",
    "    if header_element:\n",
    "        title_element = header_element.find('h1')\n",
    "        if title_element:\n",
    "            return title_element.get_text(strip=True)\n",
    "\n",
    "    title_element = soup.find('h1')\n",
    "    if title_element:\n",
    "        return title_element.get_text(strip=True)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def extract_article_date(soup):\n",
    "    date_element = soup.find('time')\n",
    "    if date_element:\n",
    "        datetime_str = date_element.get('datetime') or date_element.get('data-timestamp')\n",
    "        if datetime_str:\n",
    "            try:\n",
    "                return datetime.fromisoformat(datetime_str).strftime(\"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    date_element = soup.find(class_=re.compile('date|timestamp|publish', re.IGNORECASE))\n",
    "    if date_element:\n",
    "        datetime_str = date_element.get_text(strip=True)\n",
    "        try:\n",
    "            return datetime.strptime(datetime_str, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    script_element = soup.find('script', type='application/ld+json')\n",
    "    if script_element:\n",
    "        try:\n",
    "            ld_json = json.loads(script_element.string)\n",
    "            if 'datePublished' in ld_json:\n",
    "                return datetime.fromisoformat(ld_json['datePublished']).strftime(\"%Y-%m-%d\")\n",
    "        except (json.JSONDecodeError, KeyError, ValueError):\n",
    "            pass\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def extract_article_author(soup):\n",
    "    author_element = soup.find(class_=re.compile('author|byline', re.IGNORECASE))\n",
    "    if author_element:\n",
    "        return author_element.get_text(strip=True)\n",
    "\n",
    "    script_element = soup.find('script', type='application/ld+json')\n",
    "    if script_element:\n",
    "        try:\n",
    "            ld_json = json.loads(script_element.string)\n",
    "            if 'author' in ld_json:\n",
    "                if isinstance(ld_json['author'], dict):\n",
    "                    return ld_json['author'].get('name', '')\n",
    "                elif isinstance(ld_json['author'], list):\n",
    "                    return ', '.join([author.get('name', '') for author in ld_json['author']])\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            pass\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def extract_article_images(soup, base_url):\n",
    "    img_tags = soup.find_all('img')\n",
    "    image_urls = []\n",
    "\n",
    "    for img in img_tags:\n",
    "        img_url = img.get('src') or img.get('data-src') or img.get('srcset') or img.get('data-srcset')\n",
    "        if img_url:\n",
    "            if img_url.startswith('//'):\n",
    "                img_url = 'https:' + img_url\n",
    "            elif img_url.startswith('/'):\n",
    "                img_url = urljoin(base_url, img_url)\n",
    "            elif not img_url.startswith('http'):\n",
    "                img_url = urljoin(base_url, img_url)\n",
    "            image_urls.append(img_url)\n",
    "\n",
    "    return list(set(image_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "\n",
    "google_news = GNews(language='es', \n",
    "                    country='Mexico', \n",
    "                    period='7d', \n",
    "                    start_date=None, \n",
    "                    end_date=None, \n",
    "                    max_results=5, \n",
    "                    #exclude_websites=['yahoo.com', 'cnn.com'],\n",
    "                    #proxy=proxy\n",
    "                    )\n",
    "news = google_news.get_news('espanol Claudia Sheinbaum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_content_selenium(url,proxy=None):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run Chrome in headless mode\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36')\n",
    "    if proxy is not None:\n",
    "        options.add_argument(f'--proxy-server={proxy}')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Find the article content using common HTML tags and classes\n",
    "    article_tags = [\n",
    "        {'name': 'article'},\n",
    "        {'name': 'div', 'class': 'article-body'},\n",
    "        {'name': 'div', 'class': 'article-content'},\n",
    "        {'name': 'div', 'class': 'entry-content'},\n",
    "        {'name': 'div', 'class': 'post-content'},\n",
    "        {'name': 'div', 'class': 'story-body'},\n",
    "        {'name': 'div', 'itemprop': 'articleBody'},\n",
    "        {'name': 'div', 'id': 'article-body'},\n",
    "        {'name': 'div', 'class': 'article-text'},\n",
    "        {'name': 'div', 'class': 'post-text'},\n",
    "        {'name': 'div', 'class': 'post-body'},\n",
    "        {'name': 'div', 'class': 'rich-text'},\n",
    "        {'name': 'div', 'class': 'article-content'},\n",
    "        {'name': 'section', 'class': 'article-body'},\n",
    "        {'name': 'section', 'class': 'post-content'},\n",
    "        {'name': 'section', 'class': 'entry-content'},\n",
    "    ]\n",
    "\n",
    "    article_content = ''\n",
    "    for tag in article_tags:\n",
    "        try:\n",
    "            if 'class' in tag:\n",
    "                article_element = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, tag['class']))\n",
    "                )\n",
    "            elif 'id' in tag:\n",
    "                article_element = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.ID, tag['id']))\n",
    "                )\n",
    "            elif 'itemprop' in tag:\n",
    "                article_element = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, f\"//*[@itemprop='{tag['itemprop']}']\"))\n",
    "                )\n",
    "            else:\n",
    "                article_element = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, tag['name']))\n",
    "                )\n",
    "            \n",
    "            paragraphs = article_element.find_elements(By.TAG_NAME, 'p')\n",
    "            article_content = '\\n'.join([p.text for p in paragraphs])\n",
    "            \n",
    "            if article_content:\n",
    "                break\n",
    "        except (TimeoutException, StaleElementReferenceException):\n",
    "            continue\n",
    "\n",
    "    if not article_content:\n",
    "        # If the article content is still not found, try to extract paragraphs from the entire page\n",
    "        try:\n",
    "            paragraphs = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.TAG_NAME, 'p'))\n",
    "            )\n",
    "            article_content = '\\n'.join([p.text for p in paragraphs])\n",
    "        except (TimeoutException, StaleElementReferenceException):\n",
    "            pass\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Remove empty lines and extra whitespace\n",
    "    article_content = re.sub(r\"\\n+\", \"\\n\", article_content).strip()\n",
    "    article_content = re.sub(r\"\\s+\", \" \", article_content)\n",
    "\n",
    "    return article_content\n",
    "\n",
    "def extract_article_content_selenium_js(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run Chrome in headless mode\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load and JavaScript to render the content\n",
    "    time.sleep(5)  # Adjust the delay as needed\n",
    "\n",
    "    # Find the article content using common HTML tags and classes\n",
    "    article_tags = [\n",
    "        {'name': 'article'},\n",
    "        {'name': 'div', 'class': 'article-body'},\n",
    "        {'name': 'div', 'class': 'article-content'},\n",
    "        {'name': 'div', 'class': 'entry-content'},\n",
    "        {'name': 'div', 'class': 'post-content'},\n",
    "        {'name': 'div', 'class': 'story-body'},\n",
    "        {'name': 'div', 'itemprop': 'articleBody'},\n",
    "        {'name': 'div', 'id': 'article-body'},\n",
    "        {'name': 'div', 'class': 'article-text'},\n",
    "        {'name': 'div', 'class': 'post-text'},\n",
    "        {'name': 'div', 'class': 'post-body'},\n",
    "        {'name': 'div', 'class': 'rich-text'},\n",
    "        {'name': 'div', 'class': 'article-content'},\n",
    "        {'name': 'section', 'class': 'article-body'},\n",
    "        {'name': 'section', 'class': 'post-content'},\n",
    "        {'name': 'section', 'class': 'entry-content'},\n",
    "    ]\n",
    "\n",
    "    article_content = ''\n",
    "    for tag in article_tags:\n",
    "        try:\n",
    "            if 'class' in tag:\n",
    "                article_element = driver.find_element(By.CLASS_NAME, tag['class'])\n",
    "            elif 'id' in tag:\n",
    "                article_element = driver.find_element(By.ID, tag['id'])\n",
    "            elif 'itemprop' in tag:\n",
    "                article_element = driver.find_element(By.XPATH, f\"//*[@itemprop='{tag['itemprop']}']\")\n",
    "            else:\n",
    "                article_element = driver.find_element(By.TAG_NAME, tag['name'])\n",
    "            \n",
    "            paragraphs = article_element.find_elements(By.TAG_NAME, 'p')\n",
    "            article_content = '\\n'.join([p.text for p in paragraphs])\n",
    "            \n",
    "            if article_content:\n",
    "                break\n",
    "        except (NoSuchElementException, StaleElementReferenceException):\n",
    "            continue\n",
    "\n",
    "    if not article_content:\n",
    "        # If the article content is still not found, try to extract paragraphs from the entire page\n",
    "        try:\n",
    "            paragraphs = driver.find_elements(By.TAG_NAME, 'p')\n",
    "            article_content = '\\n'.join([p.text for p in paragraphs])\n",
    "        except (NoSuchElementException, StaleElementReferenceException):\n",
    "            pass\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Remove empty lines and extra whitespace\n",
    "    article_content = re.sub(r\"\\n+\", \"\\n\", article_content).strip()\n",
    "    article_content = re.sub(r\"\\s+\", \" \", article_content)\n",
    "\n",
    "    return article_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_article(gnews_data=None, url=None):\n",
    "    if gnews_data is None:\n",
    "        pass\n",
    "    else:\n",
    "        url = gnews_data['url']\n",
    "    \n",
    "    page_content = url\n",
    "    page_content = get_page_content(url)\n",
    "    \n",
    "    if page_content:\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        article_title = extract_article_title(soup)\n",
    "        article_date = extract_article_date(soup)\n",
    "        article_author = extract_article_author(soup)\n",
    "        article_images = extract_article_images(soup, url)\n",
    "        # Try extracting content using the existing function first\n",
    "        article_content = extract_article_content_selenium(url)\n",
    "        \n",
    "        # If the content is empty, try the new function for JavaScript-rendered content\n",
    "        if not article_content.strip():\n",
    "            article_content = extract_article_content_selenium_js(url)\n",
    "        #article_content = ''\n",
    "        \n",
    "        article_data = {\n",
    "            'url':url,\n",
    "            'title': article_title,\n",
    "            'date': article_date,\n",
    "            'author': article_author,\n",
    "            'content': article_content,\n",
    "            'images': article_images\n",
    "        }\n",
    "        if gnews_data is not None:\n",
    "            article_data['metadata'] = gnews_data\n",
    "        return article_data\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://news.google.com/rss/articles/CBMiXGh0dHBzOi8vY25uZXNwYW5vbC5jbm4uY29tLzIwMjQvMDMvMTEvY2xhdWRpYS1zaGVpbmJhdW0tcHJvcHVlc3RhLXJlbGFjaW9uLW1leGljby1lZXV1LW9yaXgv0gFgaHR0cHM6Ly9jbm5lc3Bhbm9sLmNubi5jb20vMjAyNC8wMy8xMS9jbGF1ZGlhLXNoZWluYmF1bS1wcm9wdWVzdGEtcmVsYWNpb24tbWV4aWNvLWVldXUtb3JpeC9hbXAv?oc=5&hl=en-US&gl=US&ceid=US:en',\n",
       " 'title': 'Google News',\n",
       " 'date': '',\n",
       " 'author': '',\n",
       " 'content': '',\n",
       " 'images': [],\n",
       " 'metadata': {'title': '¿Qué dijo y qué propone Sheinbaum sobre las relaciones entre EE.UU. y México? - CNN en Español',\n",
       "  'description': '¿Qué dijo y qué propone Sheinbaum sobre las relaciones entre EE.UU. y México?  CNN en Español',\n",
       "  'published date': 'Mon, 11 Mar 2024 19:27:00 GMT',\n",
       "  'url': 'https://news.google.com/rss/articles/CBMiXGh0dHBzOi8vY25uZXNwYW5vbC5jbm4uY29tLzIwMjQvMDMvMTEvY2xhdWRpYS1zaGVpbmJhdW0tcHJvcHVlc3RhLXJlbGFjaW9uLW1leGljby1lZXV1LW9yaXgv0gFgaHR0cHM6Ly9jbm5lc3Bhbm9sLmNubi5jb20vMjAyNC8wMy8xMS9jbGF1ZGlhLXNoZWluYmF1bS1wcm9wdWVzdGEtcmVsYWNpb24tbWV4aWNvLWVldXUtb3JpeC9hbXAv?oc=5&hl=en-US&gl=US&ceid=US:en',\n",
       "  'publisher': {'href': 'https://cnnespanol.cnn.com',\n",
       "   'title': 'CNN en Español'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "article_data = scrape_news_article(news[1])\n",
    "article_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
